Tokenization is one of step which comes under data preprocessing pipeline.
<br/>
Major usecase of tokenization is reduce the memory footprint for large text.
<br/>
e.g.<br/>
Let us assume sample text is<br/>
"""Jack Ma -> If you donâ€™t do it, nothing is possible.
   If you do it, at least, you have the hope that there's a chance."""
<br/>
